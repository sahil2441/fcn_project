{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "file = [\"www.google.com\",\n",
    "        \"www.facebook.com\",\n",
    "        \"www.youtube.com\",\n",
    "        \"www.baidu.com\",\n",
    "        \"www.yahoo.com\",\n",
    "        \"www.amazon.com\",\n",
    "        \"www.wikipedia.org\",\n",
    "        \"www.qq.com\",\n",
    "        \"www.taobao.com\",\n",
    "        \"www.twitter.com\",\n",
    "        \"www.google.co.in\",\n",
    "        \"www.live.com\",\n",
    "        \"www.sina.com.cn\",\n",
    "        \"www.linkedin.com\",\n",
    "        \"www.weibo.com\",\n",
    "        \"www.yahoo.co.jp\",\n",
    "        \"www.google.co.jp\",\n",
    "        \"www.google.ca\",\n",
    "        \"www.ok.ru\",\n",
    "        \"www.adcash.com\",\n",
    "        \"www.google.com.mx\",\n",
    "        \"www.diply.com\",\n",
    "        \"www.tianya.cn\",\n",
    "        \"www.google.com.hk\",\n",
    "        \"www.pornhub.com\",\n",
    "        \"www.alibaba.com\",\n",
    "        \"www.rakuten.co.jp\",\n",
    "        \"www.naver.com\",\n",
    "        \"www.amazon.co.uk\",\n",
    "        \"www.google.com.tr\",\n",
    "        \"www.adobe.com\",\n",
    "        \"www.xinhuanet.com\",\n",
    "        \"www.gmail.com\",\n",
    "        \"www.outbrain.com\",\n",
    "        \"www.xhamster.com\",\n",
    "        \"www.detail.tmall.com\",\n",
    "        \"www.soso.com\",\n",
    "        \"www.google.co.id\",\n",
    "        \"www.ebay.de\",\n",
    "        \"www.kat.cr\",\n",
    "        \"www.people.com.cn\",\n",
    "        \"www.google.pl\",\n",
    "        \"www.jd.com\",\n",
    "        \"www.cntv.cn\",\n",
    "        \"www.gmw.cn\",\n",
    "        \"www.google.com.au\",\n",
    "        \"www.go.com\",\n",
    "        \"www.nicovideo.jp\",\n",
    "        \"www.flipkart.com\",\n",
    "        \"www.cnn.com\",\n",
    "        \"www.dailymotion.com\",\n",
    "        \"www.bbc.co.uk\",\n",
    "        \"www.booking.com\",\n",
    "        \"www.github.com\",\n",
    "        \"www.googleusercontent.com\",\n",
    "        \"www.pixnet.net\",\n",
    "        \"www.dropbox.com\",\n",
    "        \"www.wikia.com\",\n",
    "        \"www.163.com\",\n",
    "        \"www.chinadaily.com.cn\"\n",
    "        ]\n",
    "\n",
    "category = {}\n",
    "category[\"www.google.com\"] = 0\n",
    "category[\"www.facebook.com\"] = 1\n",
    "category[\"www.youtube.com\"] = 2\n",
    "category[\"www.baidu.com\"] = 0\n",
    "category[\"www.yahoo.com\"] = 1\n",
    "category[\"www.amazon.com\"] = 2\n",
    "category[\"www.wikipedia.org\"] = 3\n",
    "category[\"www.twitter.com\"] = 1\n",
    "category[\"www.google.co.in\"] = 0\n",
    "category[\"www.linkedin.com\"] = 1\n",
    "category[\"www.yahoo.co.jp\"] = 1\n",
    "category[\"www.google.co.jp\"] = 0\n",
    "category[\"www.google.ca\"] = 0\n",
    "category[\"www.qq.com\"] = 2\n",
    "category[\"www.taobao.com\"] = 2\n",
    "category[\"www.live.com\"] = 1\n",
    "category[\"www.google.com.mx\"] = 0\n",
    "category[\"www.google.com.hk\"] = 0\n",
    "category[\"www.pornhub.com\"] = 2\n",
    "category[\"www.alibaba.com\"] = 2\n",
    "category[\"www.amazon.co.uk\"] = 2\n",
    "category[\"www.google.com.tr\"] = 0\n",
    "category[\"www.adobe.com\"] = 0\n",
    "category[\"www.gmail.com\"] = 0\n",
    "category[\"www.xhamster.com\"] = 2\n",
    "category[\"www.ebay.de\"] = 2\n",
    "category[\"www.google.pl\"] = 0\n",
    "category[\"www.google.com.au\"] = 0\n",
    "category[\"www.dailymotion.com\"] = 2\n",
    "category[\"www.bbc.co.uk\"] = 2\n",
    "category[\"www.google.co.id\"] = 0\n",
    "category[\"www.flipkart.com\"] = 2\n",
    "category[\"www.cnn.com\"] = 2\n",
    "category[\"www.booking.com\"] = 2\n",
    "category[\"www.github.com\"] = 0\n",
    "category[\"www.dropbox.com\"] = 0\n",
    "category[\"www.people.com.cn\"] = 2\n",
    "category[\"www.nicovideo.jp\"] = 2\n",
    "category[\"www.googleusercontent.com\"] = 0\n",
    "category[\"www.weibo.com\"] = 2\n",
    "category[\"www.sina.com.cn\"] = 2\n",
    "category[\"www.adcash.com\"] = 0\n",
    "category[\"www.ok.ru\"] = 2\n",
    "category[\"www.diply.com\"] = 2\n",
    "category[\"www.tianya.cn\"] = 2\n",
    "category[\"www.jd.com\"] = 2\n",
    "category[\"www.cntv.cn\"] = 0\n",
    "category[\"www.go.com\"] = 2\n",
    "category[\"www.naver.com\"] = 0\n",
    "category[\"www.kat.cr\"] = 0\n",
    "category[\"www.soso.com\"] = 0\n",
    "category[\"www.wikia.com\"] = 0\n",
    "category[\"www.163.com\"] = 2\n",
    "category[\"www.gmw.cn\"] = 2\n",
    "category[\"www.pixnet.net\"] = 3\n",
    "category[\"www.outbrain.com\"] = 3\n",
    "category[\"www.xinhuanet.com\"] = 2\n",
    "category[\"www.rakuten.co.jp\"] = 2\n",
    "category[\"www.detail.tmall.com\"] = 0\n",
    "category[\"www.chinadaily.com.cn\"] = 0\n",
    "\n",
    "def find_PLT(filename):\n",
    "    file = open(filename)\n",
    "    list = file.readlines()\n",
    "    mintime = 0\n",
    "    for line in list:\n",
    "        match = re.match('.*requestTime.: ([0-9^.]+)', line)\n",
    "        if match is not None:\n",
    "            mintime = float(match.groups(0)[0])\n",
    "            break\n",
    "    maxtime = 0\n",
    "    for line in list:\n",
    "        match = re.match('.*time.: ([0-9^.]+)',line)\n",
    "        if match is not None:\n",
    "            if float(match.groups(0)[0]) > maxtime:\n",
    "                maxtime = float(match.groups(0)[0])\n",
    "                \n",
    "    length = 0\n",
    "    for line in list:\n",
    "        match = re.match('.*\"len\": ([0-9]+), \"from',line)\n",
    "        if match is not None:\n",
    "            for val in match.groups(0):\n",
    "                length += int(val)                \n",
    "    \n",
    "    return (maxtime - mintime,mintime,maxtime,length)\n",
    "            \n",
    "def findcount(text, list):\n",
    "    feature = Counter()\n",
    "    for i in range(4):\n",
    "        feature[list[i]] = len(re.findall('\\.' + list[i],text))\n",
    "#         print list[i],\":\",feature[list[i]]\n",
    "    for i in range(4,len(list)):\n",
    "        feature[list[i]] = len(re.findall(list[i],text))\n",
    "    return feature\n",
    "\n",
    "\n",
    "def findAllPLT(name,count):\n",
    "    list = []\n",
    "    featureList = ['png','gif','css','js','Resource']\n",
    "    feature = Counter()\n",
    "    length = 0\n",
    "    for i in range(count):\n",
    "        filename = name + str(i)  + \".txt\"\n",
    "        text = open(filename).read()\n",
    "        feature += findcount(text,featureList)\n",
    "        (time,min,max,length) = find_PLT(filename)\n",
    "        list.append(time)\n",
    "    \n",
    "    for key in featureList:\n",
    "        if key in feature:\n",
    "            feature[key] = (feature[key]*1.0)/count\n",
    "        else:\n",
    "            feature[key] = 0\n",
    "    feature['length'] = length\n",
    "    return feature,list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = \"/Users/Aman/Desktop/Colin/data_all/results/\"\n",
    "def all_file():\n",
    "    dictionary = {}\n",
    "    for website in file:\n",
    "        feature,list = findAllPLT(filepath + website,10)\n",
    "        dictionary[website] = (feature,list)\n",
    "    return dictionary\n",
    "\n",
    "all_val_dict = all_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "file_1 = [\"www.google.com\",\n",
    "        \"www.facebook.com\",\n",
    "        \"www.youtube.com\",\n",
    "        \"www.baidu.com\",\n",
    "        \"www.yahoo.com\",\n",
    "        \"www.amazon.com\",\n",
    "        \"www.wikipedia.org\",\n",
    "        \"www.taobao.com\",\n",
    "        \"www.twitter.com\",\n",
    "        \"www.google.co.in\",\n",
    "        \"www.live.com\",\n",
    "        \"www.linkedin.com\",\n",
    "        \"www.weibo.com\",\n",
    "        \"www.yahoo.co.jp\",\n",
    "        \"www.google.co.jp\",\n",
    "        \"www.google.ca\",\n",
    "        \"www.ok.ru\",\n",
    "        \"www.adcash.com\",\n",
    "        \"www.google.com.mx\",\n",
    "        \"www.diply.com\",\n",
    "        \"www.tianya.cn\",\n",
    "        \"www.google.com.hk\",\n",
    "        \"www.pornhub.com\",\n",
    "        \"www.alibaba.com\",\n",
    "        \"www.naver.com\",\n",
    "        \"www.amazon.co.uk\",\n",
    "        \"www.google.com.tr\",\n",
    "        \"www.adobe.com\",\n",
    "        \"www.gmail.com\",\n",
    "        \"www.outbrain.com\",\n",
    "        \"www.xhamster.com\",\n",
    "        \"www.detail.tmall.com\",\n",
    "        \"www.soso.com\",\n",
    "        \"www.google.co.id\",\n",
    "        \"www.ebay.de\",\n",
    "        \"www.kat.cr\",\n",
    "        \"www.google.pl\",\n",
    "        \"www.cntv.cn\",\n",
    "        \"www.gmw.cn\",\n",
    "        \"www.google.com.au\",\n",
    "        \"www.go.com\",\n",
    "        \"www.nicovideo.jp\",\n",
    "        \"www.flipkart.com\",\n",
    "        \"www.cnn.com\",\n",
    "        \"www.dailymotion.com\",\n",
    "        \"www.bbc.co.uk\",\n",
    "        \"www.booking.com\",\n",
    "        \"www.googleusercontent.com\",\n",
    "        \"www.pixnet.net\",\n",
    "        \"www.dropbox.com\",\n",
    "        \"www.wikia.com\",\n",
    "        \"www.chinadaily.com.cn\"\n",
    "        ]\n",
    "def pageloaditeration():\n",
    "    all_val_dict['www.tianya.cn'][1].sort()\n",
    "    all_val_dict['www.tianya.cn'][1].reverse()\n",
    "    all_val_dict['www.adobe.com'][1].reverse()\n",
    "    all_val_dict['www.pixnet.net'][1].reverse()\n",
    "\n",
    "    \n",
    "    for website in file_1:\n",
    "        if website in all_val_dict.keys():\n",
    "            if max(all_val_dict[website][1]) < 8 and min(all_val_dict[website][1])  > 0:\n",
    "#                 all_val_dict[website][1].sort()\n",
    "#                 all_val_dict[website][1].reverse()\n",
    "                plt.plot(all_val_dict[website][1],linewidth=2.0)\n",
    "\n",
    "    plt.ylabel('Time in seconds ---->')\n",
    "    plt.xlabel('Iterations ---->')\n",
    "    plt.ylim(ymax=8)\n",
    "    plt.xlim(xmax=7)\n",
    "    plt.grid(True)\n",
    "    plt.title(\"PAGELOAD TIME  vs  ITERATION\")\n",
    "    plt.show()\n",
    "\n",
    "def scatterplot(file):\n",
    "    for website in file:\n",
    "       plt.scatter(sum(all_val_dict[website][0].values()),np.mean(all_val_dict[website][1]),alpha=0.5,color=\"blue\")\n",
    "\n",
    "    plt.ylabel('Time in seconds ---->')\n",
    "    plt.xlabel('Number of objects ---->')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"PAGELOAD TIME  vs  #OBJECTS\")\n",
    "    plt.show()\n",
    "\n",
    "def pageloadvsobj(all_val_dict):\n",
    "    for website in file:\n",
    "        sum = 0.0\n",
    "        i = 0\n",
    "        for key in all_val_dict[website][0]:\n",
    "            sum += regr.coef_[i]* all_val_dict[website][0][key]\n",
    "            i += 1\n",
    "            sum = max(sum,2)\n",
    "            plt.scatter(sum,np.mean(all_val_dict[website][1]),alpha=0.5)\n",
    "    \n",
    "    plt.ylabel('Time in seconds ---->')\n",
    "    plt.xlabel('Number of objects ---->')\n",
    "    plt.grid(True)\n",
    "    plt.xlim(xmin=-5)\n",
    "    plt.ylim(ymin=0)\n",
    "    plt.title(\"PAGELOAD TIME  vs  #OBJECTS\")\n",
    "    plt.show()\n",
    "\n",
    "def sp():\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    yaxis = []\n",
    "    keys = ['png','gif','css','js']\n",
    "    for website in file:\n",
    "        sum = 0\n",
    "        for v in keys:\n",
    "            sum += all_val_dict[website][0][v]\n",
    "        x.append(sum)\n",
    "        y.append(all_val_dict[website][0][\"length\"])\n",
    "        z.append(all_val_dict[website][0][\"Resource\"])\n",
    "        yaxis.append(np.mean(all_val_dict[website][1]))\n",
    "    \n",
    "    x = np.array(ss.zscore(x))\n",
    "    y = np.array(ss.zscore(y))\n",
    "    z = np.array(ss.zscore(z))\n",
    "   \n",
    "    colors = iter(cm.rainbow(np.linspace(0, 1, 3)))\n",
    "\n",
    "    ax1.scatter(x,yaxis , s=35,color=next(colors), marker=\"o\",edgecolor = 'black', label='#Objects')\n",
    "    ax1.scatter(y,yaxis , s=35,color=next(colors), marker=\"s\",edgecolor = 'black', label='LengthWebsite')\n",
    "    ax1.scatter(z,yaxis , s=35,color=next(colors), marker=\"p\",edgecolor = 'black', label='#Resources Contacted')\n",
    "    plt.legend(loc='upper left');\n",
    "    plt.colors()\n",
    "    plt.ylabel('Time in seconds ---->')\n",
    "    plt.xlabel('Features(NORMALIZED TO FIT/COMPARE) ---->')\n",
    "    plt.grid(True)\n",
    "    plt.title(\"PAGELOAD TIME vs Features\")\n",
    "    plt.show()\n",
    "\n",
    "# pageloaditeration()\n",
    "# pageloadvsobj(all_val_dict)\n",
    "# scatterplot(file)\n",
    "sp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from scipy import stats as ss\n",
    "\n",
    "def regression(Y,X):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.array(ss.zscore(Y)).T\n",
    "#     X = np.array([x + 0.1001 for x in X])\n",
    "    X = np.array([(ss.zscore(x)) for x in X.T]).T\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X,Y)\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    print(\"Residual sum of squares: %.2f\"% np.mean((regr.predict(X) - Y) ** 2))\n",
    "    \n",
    "    return regr    \n",
    "\n",
    "def getValueRegr(all_val_dict):\n",
    "    Y = []\n",
    "    X = []\n",
    "    for website in file:\n",
    "        if np.sum(all_val_dict[website][1]) == 0:\n",
    "            continue\n",
    "        Y.append(np.mean(all_val_dict[website][1][4]))\n",
    "        x1 = all_val_dict[website][0].values()\n",
    "        \n",
    "        X.append(x1)\n",
    "    \n",
    "    return (Y,X)\n",
    "\n",
    "Y,X = getValueRegr(all_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.9, 0, 0, 957684, 3.8, 0], [27.1, 27.0, 4.0, 1180311, 8.1, 24.0], [95.0, 90.3, 5.0, 2689962, 16.0, 30.6], [17.0, 27.2, 1.0, 574237, 6.0, 4.0], [49.1, 168.2, 9.8, 3301150, 4.9, 52.4], [121.6, 118.4, 9.4, 3996617, 29.9, 117.1], [8.0, 16.2, 0, 116925, 7.0, 0], [138.4, 46.4, 6.7, 2856251, 38.0, 3.2], [62.2, 157.5, 4.3, 0, 17.1, 42.0], [81.2, 31.6, 2.0, 5018456, 20.6, 18.8], [8.6, 0, 0, 983638, 3.2, 0], [4.0, 11.7, 0, 218508, 4.0, 0], [89.9, 44.6, 17.4, 0, 7.0, 3.8], [17.0, 8.0, 0, 775049, 0, 0], [2.0, 5.0, 0, 32875, 0, 0], [65.9, 66.6, 8.8, 800267, 9.5, 5.0], [8.7, 0, 0, 946612, 4.3, 0], [9.8, 0, 0, 944953, 4.2, 0], [77.0, 42.9, 0, 2625007, 6.0, 16.4], [155.7, 241.4, 38.0, 6704893, 18.0, 163.0], [10.6, 0, 0, 677052, 3.2, 0], [126.0, 98.5, 2.0, 7031619, 10.3, 20.0], [37.9, 41.5, 4.0, 481585, 10.0, 8.0], [7.4, 0, 0, 896770, 4.1, 0], [91.8, 112.2, 44.2, 5168742, 10.4, 42.4], [45.7, 127.2, 2.8, 1630601, 5.5, 94.0], [298.3, 137.5, 108.0, 10987159, 10.3, 5.1], [69.6, 46.2, 44.1, 1370636, 14.4, 24.0], [250.8, 105.9, 15.0, 9925895, 26.4, 76.6], [10.3, 0, 0, 944766, 4.3, 0], [107.5, 125.9, 1.1, 4279627, 27.5, 0], [125.6, 110.4, 27.0, 2239826, 24.3, 3.6], [43.0, 44.5, 2.0, 1899608, 17.0, 14.0], [92.6, 67.3, 2.7, 3097985, 23.0, 4.0], [95.4, 17.1, 49.0, 1093558, 26.0, 6.0], [20.0, 37.5, 5.0, 343571, 4.0, 3.0], [10.2, 0, 0, 981274, 3.2, 0], [168.2, 29.2, 2.1, 5013434, 5.8, 4.0], [26.8, 21.0, 1.0, 1383866, 5.0, 11.0], [157.2, 47.4, 21.6, 0, 11.7, 4.5], [11.0, 0, 0, 983865, 3.4, 0], [24.1, 38.6, 2.4, 1283892, 0.6, 2.3], [20.0, 11.3, 0, 144436, 26.0, 4.0], [8.9, 0, 0, 944636, 4.2, 0], [36.0, 35.8, 0, 3261451, 4.0, 35.0], [94.3, 138.7, 1.8, 2851292, 35.1, 56.7], [53.4, 16.6, 0.2, 1520321, 17.0, 4.0], [99.8, 94.7, 5.6, 3162296, 22.5, 8.0], [115.6, 158.3, 5.7, 4483332, 17.6, 28.8], [72.9, 61.7, 4.1, 3114011, 17.0, 28.0], [3.0, 0, 0, 11058, 2.0, 0], [200.6, 163.9, 10.3, 7401093, 57.3, 16.0], [131.6, 92.8, 11.7, 6207764, 45.0, 50.4], [117.6, 84.9, 6.0, 4627729, 14.4, 5.0], [53.1, 45.3, 56.0, 1552584, 11.4, 0], [51.8, 35.3, 10.4, 1647468, 3.6, 3.6]]\n",
      "('Coefficients: \\n', array([ 0.48527886,  0.31057993,  0.04810704, -0.19452292,  0.36876282,\n",
      "       -0.3931335 ]))\n",
      "Residual sum of squares: 0.41\n",
      "[49.1, 168.2, 9.8, 3301150, 4.9, 52.4]\n",
      "[-0.44721992 -0.44712311 -0.44725186  2.23606798 -0.44725585 -0.44721724]\n",
      "[-0.80149151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aman/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print X\n",
    "regr = regression(Y,X)\n",
    "test = all_val_dict['www.yahoo.com'][0].values()\n",
    "print test\n",
    "# test = test[1:-1]\n",
    "test = np.array(ss.zscore(test)).T\n",
    "print test\n",
    "print regr.predict(test)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
